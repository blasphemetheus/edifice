# Generative Models with Edifice

## Setup

**Choose one of the two cells below** depending on how you started Livebook.

### Standalone (default)

Use this if you started Livebook normally (`livebook server`).
Uncomment the EXLA lines for GPU acceleration.

```elixir
edifice_dep =
  if File.dir?(Path.expand("~/edifice")) do
    {:edifice, path: Path.expand("~/edifice")}
  else
    {:edifice, "~> 0.2.0"}
  end

Mix.install([
  edifice_dep,
  # {:exla, "~> 0.10"},
  {:kino_vega_lite, "~> 0.1"},
  {:kino, "~> 0.14"}
])

# Nx.global_default_backend(EXLA.Backend)
alias VegaLite, as: Vl
```

### Attached to project (recommended for Nix/CUDA)

Use this if you started Livebook via `./scripts/livebook.sh`.
See the Architecture Zoo notebook for full setup instructions.

```elixir
Nx.global_default_backend(EXLA.Backend)
alias VegaLite, as: Vl
IO.puts("Attached mode — using EXLA backend from project node")
```

## Introduction

Generative models learn to **produce new data** that resembles the training
distribution. Unlike classifiers that map inputs to labels, generative models
learn the underlying data distribution and can sample from it.

This notebook trains a **Variational Autoencoder (VAE)** on 2D point clouds
and visualizes how it learns to compress data into a latent space and
reconstruct it.

**What you'll learn:**

* How VAEs encode data into a latent distribution (mu + log_var)
* The reparameterization trick for differentiable sampling
* Training with reconstruction loss + KL divergence
* Visualizing the learned latent space
* Generating new samples by decoding random latent points

## Generate 2D Data

We create a dataset of 2D points arranged in a crescent moon shape.
The VAE must learn this non-trivial distribution.

```elixir
IO.puts("Generating crescent moon dataset...")

n_points = 1500
key = Nx.Random.key(42)

# Upper crescent
{angles_upper, key} = Nx.Random.uniform(key, shape: {n_points})
angles_upper = Nx.multiply(angles_upper, :math.pi())
{noise_u, key} = Nx.Random.normal(key, shape: {n_points, 2})

upper_x = Nx.add(Nx.cos(angles_upper), Nx.multiply(noise_u[[.., 0]], 0.1))
upper_y = Nx.add(Nx.sin(angles_upper), Nx.multiply(noise_u[[.., 1]], 0.1))
upper = Nx.stack([upper_x, upper_y], axis: 1)

# Lower crescent (shifted and flipped)
{angles_lower, key} = Nx.Random.uniform(key, shape: {n_points})
angles_lower = Nx.multiply(angles_lower, :math.pi())
{noise_l, key} = Nx.Random.normal(key, shape: {n_points, 2})

lower_x = Nx.add(Nx.subtract(1.0, Nx.cos(angles_lower)), Nx.multiply(noise_l[[.., 0]], 0.1))
lower_y = Nx.subtract(Nx.subtract(0.0, Nx.sin(angles_lower)), Nx.add(0.5, Nx.multiply(noise_l[[.., 1]], 0.1)))
lower = Nx.stack([lower_x, lower_y], axis: 1)

data = Nx.concatenate([upper, lower])
labels = Nx.concatenate([Nx.broadcast(0, {n_points}), Nx.broadcast(1, {n_points})])

# Shuffle
n_total = 2 * n_points
{shuffle_noise, _key} = Nx.Random.uniform(Nx.Random.key(99), shape: {n_total})
shuffle_idx = Nx.argsort(shuffle_noise)
data = Nx.take(data, shuffle_idx)
labels = Nx.take(labels, shuffle_idx)

# Batch for training
batch_size = 64
IO.puts("  Batching #{n_total} points into batches of #{batch_size}...")

train_batches =
  Nx.to_batched(data, batch_size)
  |> Enum.to_list()
  |> Enum.map(fn batch -> {batch, batch} end)

IO.puts("Ready: #{n_total} points, #{length(train_batches)} batches/epoch")
```

```elixir
chart_data =
  Enum.zip_with(
    [Nx.to_flat_list(data[[.., 0]]), Nx.to_flat_list(data[[.., 1]]), Nx.to_flat_list(labels)],
    fn [x, y, l] -> %{"x" => x, "y" => y, "class" => if(trunc(l) == 0, do: "upper", else: "lower")} end
  )

Vl.new(width: 500, height: 350, title: "Training Data: Two Crescents")
|> Vl.data_from_values(chart_data)
|> Vl.mark(:circle, size: 15, opacity: 0.5)
|> Vl.encode_field(:x, "x", type: :quantitative)
|> Vl.encode_field(:y, "y", type: :quantitative)
|> Vl.encode_field(:color, "class", type: :nominal)
```

## Build the VAE

We build the VAE as a **single Axon graph** so we can use the standard
`Axon.Loop.trainer/3`. The model outputs a container with the reconstruction,
mu, and log_var — the custom loss uses all three.

We use a **2D latent space** so we can visualize it directly.

```elixir
IO.puts("Building VAE (2D latent space)...")

latent_size = 2
input_size = 2

# Encoder
input = Axon.input("input", shape: {nil, input_size})

enc =
  input
  |> Axon.dense(64, name: "enc_0")
  |> Axon.activation(:relu)
  |> Axon.dense(32, name: "enc_1")
  |> Axon.activation(:relu)

mu = Axon.dense(enc, latent_size, name: "mu")
log_var = Axon.dense(enc, latent_size, name: "log_var")

# Decoder (uses mu as latent vector — deterministic for stable training)
recon =
  mu
  |> Axon.dense(32, name: "dec_0")
  |> Axon.activation(:relu)
  |> Axon.dense(64, name: "dec_1")
  |> Axon.activation(:relu)
  |> Axon.dense(input_size, name: "dec_out")

# Combined model outputs reconstruction + distribution params
vae_model = Axon.container(%{reconstruction: recon, mu: mu, log_var: log_var})

IO.puts("  Encoder: {batch, 2} -> mu {batch, 2} + log_var {batch, 2}")
IO.puts("  Decoder: mu {batch, 2} -> reconstruction {batch, 2}")
```

## Train the VAE

VAE training uses a custom loss: **reconstruction error + KL divergence**.
The KL term regularizes the latent space toward a standard normal N(0, I),
which is what makes sampling and generation possible.

```elixir
# Custom VAE loss: reconstruction MSE + beta * KL divergence
vae_loss = fn y_pred, y_true ->
  recon_loss = Nx.mean(Nx.pow(Nx.subtract(y_pred.reconstruction, y_true), 2))

  kl =
    Nx.subtract(
      Nx.add(1.0, y_pred.log_var),
      Nx.add(Nx.pow(y_pred.mu, 2), Nx.exp(y_pred.log_var))
    )
    |> Nx.sum(axes: [-1])
    |> Nx.mean()
    |> Nx.multiply(-0.5)

  Nx.add(recon_loss, kl)
end

# Increase to 30-50 for better results; 10 is enough to see learning
IO.puts("Training VAE (10 epochs)...")

trained_state =
  vae_model
  |> Axon.Loop.trainer(vae_loss, Polaris.Optimizers.adam(learning_rate: 1.0e-3))
  |> Axon.Loop.run(train_batches, Axon.ModelState.empty(), epochs: 10)

IO.puts("Training complete!")
```

## Visualize the Latent Space

The encoder maps each data point to a 2D latent vector (mu).
Let's see how the two crescents are organized in latent space.

```elixir
IO.puts("Encoding all data points into latent space...")

{_init_fn, predict_fn} = Axon.build(vae_model)
output = predict_fn.(trained_state, data)

latent_data =
  Enum.zip_with(
    [Nx.to_flat_list(output.mu[[.., 0]]),
     Nx.to_flat_list(output.mu[[.., 1]]),
     Nx.to_flat_list(labels)],
    fn [z1, z2, l] ->
      %{"z1" => z1, "z2" => z2, "class" => if(trunc(l) == 0, do: "upper", else: "lower")}
    end
  )

Vl.new(width: 500, height: 350, title: "Latent Space (encoder mu)")
|> Vl.data_from_values(latent_data)
|> Vl.mark(:circle, size: 15, opacity: 0.5)
|> Vl.encode_field(:x, "z1", type: :quantitative, title: "z1")
|> Vl.encode_field(:y, "z2", type: :quantitative, title: "z2")
|> Vl.encode_field(:color, "class", type: :nominal)
```

## Generate New Samples

We build a standalone decoder that shares weights with the trained model,
sample random points from the latent space prior N(0, I), and decode
them into new data points.

```elixir
IO.puts("Building standalone decoder for generation...")

# Build a decoder model that matches the trained weights
latent_input = Axon.input("latent", shape: {nil, latent_size})

standalone_decoder =
  latent_input
  |> Axon.dense(32, name: "dec_0")
  |> Axon.activation(:relu)
  |> Axon.dense(64, name: "dec_1")
  |> Axon.activation(:relu)
  |> Axon.dense(input_size, name: "dec_out")

{_dec_init, dec_predict_fn} = Axon.build(standalone_decoder)

# The trained_state already has "dec_0", "dec_1", "dec_out" keys
# which match the standalone decoder's layer names

IO.puts("Generating new samples from latent space...")

n_samples = 500
{z_samples, _k} = Nx.Random.normal(Nx.Random.key(123), shape: {n_samples, latent_size})

generated = dec_predict_fn.(trained_state, %{"latent" => z_samples})

gen_data =
  Enum.zip_with(
    [Nx.to_flat_list(generated[[.., 0]]), Nx.to_flat_list(generated[[.., 1]])],
    fn [x, y] -> %{"x" => x, "y" => y, "source" => "generated"} end
  )

real_data =
  Enum.zip_with(
    [Nx.to_flat_list(data[[.., 0]]), Nx.to_flat_list(data[[.., 1]])],
    fn [x, y] -> %{"x" => x, "y" => y, "source" => "real"} end
  )

Vl.new(width: 500, height: 350, title: "Real vs Generated Samples")
|> Vl.data_from_values(real_data ++ gen_data)
|> Vl.mark(:circle, size: 15, opacity: 0.4)
|> Vl.encode_field(:x, "x", type: :quantitative)
|> Vl.encode_field(:y, "y", type: :quantitative)
|> Vl.encode_field(:color, "source", type: :nominal)
```

## Decode a Grid of Latent Points

By decoding a uniform grid across the latent space, we can see how
the decoder maps different latent regions to data space.

```elixir
IO.puts("Decoding latent space grid...")

grid_res = 20
grid_range = Enum.map(0..(grid_res - 1), fn i -> -3.0 + 6.0 * i / (grid_res - 1) end)

grid_z =
  for z1 <- grid_range, z2 <- grid_range do
    [z1, z2]
  end

grid_tensor = Nx.tensor(grid_z)
decoded_grid = dec_predict_fn.(trained_state, %{"latent" => grid_tensor})

grid_data =
  Enum.zip_with(
    [Nx.to_flat_list(decoded_grid[[.., 0]]),
     Nx.to_flat_list(decoded_grid[[.., 1]]),
     Enum.map(grid_z, fn [z1, _] -> z1 end),
     Enum.map(grid_z, fn [_, z2] -> z2 end)],
    fn [x, y, z1, z2] ->
      %{"x" => x, "y" => y, "z1" => Float.round(z1, 1), "z2" => Float.round(z2, 1)}
    end
  )

Vl.new(width: 500, height: 350, title: "Decoded Latent Grid (color = z1 position)")
|> Vl.data_from_values(grid_data)
|> Vl.mark(:circle, size: 30, opacity: 0.6)
|> Vl.encode_field(:x, "x", type: :quantitative, title: "Decoded x")
|> Vl.encode_field(:y, "y", type: :quantitative, title: "Decoded y")
|> Vl.encode_field(:color, "z1", type: :quantitative, scale: %{scheme: "viridis"}, title: "z1")
```

## Key Takeaways

1. **Encode → Sample → Decode**: The VAE learns a compressed latent
   representation. The encoder outputs a *distribution* (mu, log_var),
   not a single point — this is what makes generation possible.

2. **KL divergence regularizes**: Without KL, the encoder would just
   memorize inputs (autoencoder). KL pushes the latent distribution
   toward N(0,I), ensuring the decoder can handle random samples.

3. **2D latent space = direct visualization**: With latent_size=2,
   we can literally plot where each data point lands in latent space
   and see the decoder's mapping from latent to data space.

4. **Beta controls the trade-off**: `beta=1.0` is standard ELBO.
   Lower beta gives sharper reconstructions; higher beta gives a
   smoother, more structured latent space (beta-VAE).

## What's Next?

* **Try higher-dimensional latent spaces**: `latent_size: 8` or `16` for
  more capacity — you'll need dimensionality reduction (PCA/t-SNE) to
  visualize the latent space.
* **Normalizing Flow**: `Edifice.build(:normalizing_flow, ...)` gives
  exact log-likelihood instead of the ELBO approximation.
* **VQ-VAE**: Discrete latent codes via `Edifice.build(:vq_vae, ...)` —
  often produces sharper results.
* **Diffusion models**: See the Diffusion notebook for step-by-step
  denoising with `Edifice.build(:dit, ...)`.
