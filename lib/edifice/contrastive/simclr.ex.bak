defmodule Edifice.Contrastive.SimCLR do
  @moduledoc """
  SimCLR - Simple Contrastive Learning of Representations.

  Implements the SimCLR framework from "A Simple Framework for Contrastive
  Learning of Visual Representations" (Chen et al., ICML 2020). SimCLR learns
  representations by maximizing agreement between differently augmented views
  of the same data example via a contrastive loss in the latent space.

  ## Key Innovations

  - **Composition of augmentations**: Random augmentations create positive pairs
  - **Projection head**: A nonlinear MLP head maps representations to a space
    where contrastive loss is applied (discarded after training)
  - **NT-Xent loss**: Normalized temperature-scaled cross-entropy loss
  - **Large batch contrastive**: Uses other examples in the batch as negatives

  ## Architecture

  ```
  Augmented View 1     Augmented View 2
        |                     |
        v                     v
  +------------+        +------------+
  |  Encoder   |        |  Encoder   |  (shared weights)
  +------------+        +------------+
        |                     |
        v                     v
  +------------+        +------------+
  | Projection |        | Projection |  (shared weights, MLP)
  |    Head    |        |    Head    |
  +------------+        +------------+
        |                     |
        v                     v
       z_i                   z_j
        |                     |
        +--------->.<---------+
              NT-Xent Loss
  ```

  ## Usage

      # Build the full SimCLR model (encoder + projection head)
      model = SimCLR.build(encoder_dim: 287, projection_dim: 128)

      # After training, discard projection head and use encoder output
      # for downstream tasks

      # Compute contrastive loss between two batches of embeddings
      loss = SimCLR.nt_xent_loss(z_i, z_j, temperature: 0.5)

  ## References
  - Paper: https://arxiv.org/abs/2002.05709
  """

  require Axon
  import Nx.Defn

  # ============================================================================
  # Default Hyperparameters
  # ============================================================================

  @doc "Default projection head output dimension"
  def default_projection_dim, do: 128

  @doc "Default projection head hidden dimension"
  def default_hidden_dim, do: 256

  @doc "Default contrastive loss temperature"
  def default_temperature, do: 0.5

  # ============================================================================
  # Model Building
  # ============================================================================

  @doc """
  Build a SimCLR model (encoder + projection head).

  ## Options
    - `:encoder_dim` - Input/encoder feature dimension (required)
    - `:projection_dim` - Projection head output dimension (default: 128)
    - `:hidden_dim` - Projection head hidden dimension (default: 256)

  ## Returns
    An Axon model that maps inputs to normalized projection embeddings.
  """
  @spec build(keyword()) :: Axon.t()
  def build(opts \\ []) do
    encoder_dim = Keyword.fetch!(opts, :encoder_dim)
    projection_dim = Keyword.get(opts, :projection_dim, default_projection_dim())
    hidden_dim = Keyword.get(opts, :hidden_dim, default_hidden_dim())

    # Input: [batch, encoder_dim]
    input = Axon.input("features", shape: {nil, encoder_dim})

    # Encoder: 2-layer MLP with ReLU
    encoder =
      input
      |> Axon.dense(hidden_dim, name: "encoder_fc1")
      |> Axon.activation(:relu, name: "encoder_relu1")
      |> Axon.layer_norm(name: "encoder_norm")
      |> Axon.dense(hidden_dim, name: "encoder_fc2")
      |> Axon.activation(:relu, name: "encoder_relu2")

    # Projection head: dense -> relu -> dense (output used for contrastive loss)
    encoder
    |> Axon.dense(hidden_dim, name: "proj_fc1")
    |> Axon.activation(:relu, name: "proj_relu")
    |> Axon.dense(projection_dim, name: "proj_fc2")
  end

  @doc """
  Build only the encoder portion (for downstream use after pretraining).

  ## Options
    - `:encoder_dim` - Input feature dimension (required)
    - `:hidden_dim` - Encoder hidden dimension (default: 256)

  ## Returns
    An Axon model that maps inputs to encoder representations.
  """
  @spec build_encoder(keyword()) :: Axon.t()
  def build_encoder(opts \\ []) do
    encoder_dim = Keyword.fetch!(opts, :encoder_dim)
    hidden_dim = Keyword.get(opts, :hidden_dim, default_hidden_dim())

    input = Axon.input("features", shape: {nil, encoder_dim})

    input
    |> Axon.dense(hidden_dim, name: "encoder_fc1")
    |> Axon.activation(:relu, name: "encoder_relu1")
    |> Axon.layer_norm(name: "encoder_norm")
    |> Axon.dense(hidden_dim, name: "encoder_fc2")
    |> Axon.activation(:relu, name: "encoder_relu2")
  end

  # ============================================================================
  # NT-Xent Loss
  # ============================================================================

  @doc """
  Compute the NT-Xent (Normalized Temperature-scaled Cross-Entropy) loss.

  Given two batches of L2-normalized embeddings from augmented views,
  computes the contrastive loss that pulls positive pairs together
  and pushes negative pairs apart.

  ## Parameters
    - `z_i` - Embeddings from view 1: [batch, projection_dim]
    - `z_j` - Embeddings from view 2: [batch, projection_dim]

  ## Options
    - `:temperature` - Temperature scaling parameter (default: 0.5)

  ## Returns
    Scalar loss tensor.
  """
  @spec nt_xent_loss(Nx.Tensor.t(), Nx.Tensor.t(), keyword()) :: Nx.Tensor.t()
  defn nt_xent_loss(z_i, z_j, opts \\ []) do
    temperature = opts[:temperature] || 0.5

    # L2 normalize embeddings
    z_i = l2_normalize(z_i)
    z_j = l2_normalize(z_j)

    batch_size = Nx.axis_size(z_i, 0)

    # Concatenate: [2*batch, dim]
    z = Nx.concatenate([z_i, z_j], axis: 0)

    # Similarity matrix: [2*batch, 2*batch]
    sim = Nx.dot(z, [1], z, [1]) / temperature

    # Create labels: positive pairs are (i, i+batch) and (i+batch, i)
    labels_top = Nx.iota({batch_size}) + batch_size
    labels_bottom = Nx.iota({batch_size})
    labels = Nx.concatenate([labels_top, labels_bottom])

    # Mask out self-similarity (diagonal)
    n = 2 * batch_size
    diag_mask = Nx.equal(Nx.iota({n, 1}), Nx.iota({1, n}))
    large_neg = Nx.Constants.neg_infinity(Nx.type(sim))
    sim = Nx.select(diag_mask, large_neg, sim)

    # Cross-entropy loss (log-softmax + nll)
    log_softmax = sim - Nx.log(Nx.sum(Nx.exp(sim - Nx.reduce_max(sim, axes: [1], keep_axes: true)), axes: [1], keep_axes: true)) - Nx.reduce_max(sim, axes: [1], keep_axes: true)

    # Gather the positive pair log-probabilities
    # For each row i, the positive is at column labels[i]
    pos_log_probs = gather_along_axis(log_softmax, labels)

    -Nx.mean(pos_log_probs)
  end

  defnp l2_normalize(x) do
    norm = Nx.sqrt(Nx.sum(x * x, axes: [1], keep_axes: true) + 1.0e-8)
    x / norm
  end

  defnp gather_along_axis(matrix, indices) do
    # Gather one element per row using the index vector
    n = Nx.axis_size(matrix, 0)
    rows = Nx.iota({n})
    Nx.gather(matrix, Nx.stack([rows, indices], axis: 1))
  end

  # ============================================================================
  # Utilities
  # ============================================================================

  @doc """
  Get the output size of the SimCLR model.
  """
  @spec output_size(keyword()) :: non_neg_integer()
  def output_size(opts \\ []) do
    Keyword.get(opts, :projection_dim, default_projection_dim())
  end
end
