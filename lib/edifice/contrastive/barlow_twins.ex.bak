defmodule Edifice.Contrastive.BarlowTwins do
  @moduledoc """
  Barlow Twins - Self-Supervised Learning via Redundancy Reduction.

  Implements Barlow Twins from "Barlow Twins: Self-Supervised Learning via
  Redundancy Reduction" (Zbontar et al., ICML 2021). Barlow Twins learns
  representations by making the cross-correlation matrix between the embeddings
  of two distorted versions of a sample close to the identity matrix.

  ## Key Innovations

  - **Redundancy reduction**: Decorrelates embedding dimensions
  - **No negative pairs**: Unlike SimCLR, does not need large batches for negatives
  - **No asymmetry**: Unlike BYOL, uses a symmetric architecture
  - **Cross-correlation objective**: Pushes toward identity cross-correlation

  ## Loss Function

  The Barlow Twins loss has two terms:
  1. **Invariance**: Diagonal of cross-correlation should be 1 (same feature
     should be similar across views)
  2. **Redundancy reduction**: Off-diagonal should be 0 (different features
     should be decorrelated)

  ```
  L = sum_i (1 - C_ii)^2 + lambda * sum_{i!=j} C_ij^2
  ```

  where C is the cross-correlation matrix between batch-normalized embeddings.

  ## Architecture

  ```
  Augmented View 1         Augmented View 2
        |                         |
        v                         v
  +------------+           +------------+
  |  Encoder   |           |  Encoder   |  (shared weights)
  +------------+           +------------+
        |                         |
        v                         v
  +------------+           +------------+
  | Projector  |           | Projector  |  (shared weights)
  +------------+           +------------+
        |                         |
        v                         v
       z_A                       z_B
        |                         |
        +-------> C = z_A^T z_B <-+
                  Barlow Loss
  ```

  ## Usage

      model = BarlowTwins.build(encoder_dim: 287, projection_dim: 256)

      # Compute loss between two batches of projections
      loss = BarlowTwins.barlow_loss(z_a, z_b, lambda: 0.005)

  ## References
  - Paper: https://arxiv.org/abs/2103.03230
  """

  require Axon
  import Nx.Defn

  # ============================================================================
  # Default Hyperparameters
  # ============================================================================

  @doc "Default projection head output dimension"
  def default_projection_dim, do: 256

  @doc "Default projection head hidden dimension"
  def default_hidden_dim, do: 512

  @doc "Default redundancy reduction coefficient"
  def default_lambda, do: 0.005

  # ============================================================================
  # Model Building
  # ============================================================================

  @doc """
  Build a Barlow Twins model (encoder + projector).

  ## Options
    - `:encoder_dim` - Input feature dimension (required)
    - `:projection_dim` - Projector output dimension (default: 256)
    - `:hidden_dim` - Hidden dimension for encoder and projector (default: 512)

  ## Returns
    An Axon model mapping inputs to projection embeddings.
  """
  @spec build(keyword()) :: Axon.t()
  def build(opts \\ []) do
    encoder_dim = Keyword.fetch!(opts, :encoder_dim)
    projection_dim = Keyword.get(opts, :projection_dim, default_projection_dim())
    hidden_dim = Keyword.get(opts, :hidden_dim, default_hidden_dim())

    input = Axon.input("features", shape: {nil, encoder_dim})

    # Encoder
    encoded =
      input
      |> Axon.dense(hidden_dim, name: "encoder_fc1")
      |> Axon.activation(:relu, name: "encoder_relu1")
      |> Axon.layer_norm(name: "encoder_norm1")
      |> Axon.dense(hidden_dim, name: "encoder_fc2")
      |> Axon.activation(:relu, name: "encoder_relu2")
      |> Axon.layer_norm(name: "encoder_norm2")

    # Projector (3-layer MLP as in the paper)
    encoded
    |> Axon.dense(hidden_dim, name: "proj_fc1")
    |> Axon.activation(:relu, name: "proj_relu1")
    |> Axon.layer_norm(name: "proj_norm1")
    |> Axon.dense(hidden_dim, name: "proj_fc2")
    |> Axon.activation(:relu, name: "proj_relu2")
    |> Axon.layer_norm(name: "proj_norm2")
    |> Axon.dense(projection_dim, name: "proj_fc3")
  end

  # ============================================================================
  # Barlow Twins Loss
  # ============================================================================

  @doc """
  Compute the Barlow Twins loss.

  Computes the cross-correlation matrix between batch-normalized embeddings
  of two views and pushes it toward the identity matrix.

  ## Parameters
    - `z_a` - Embeddings from view A: [batch, projection_dim]
    - `z_b` - Embeddings from view B: [batch, projection_dim]

  ## Options
    - `:lambda` - Off-diagonal loss weight (default: 0.005)

  ## Returns
    Scalar loss tensor.
  """
  @spec barlow_loss(Nx.Tensor.t(), Nx.Tensor.t(), keyword()) :: Nx.Tensor.t()
  defn barlow_loss(z_a, z_b, opts \\ []) do
    lambda = opts[:lambda] || 0.005

    # Batch normalize along the batch dimension
    z_a_norm = batch_normalize(z_a)
    z_b_norm = batch_normalize(z_b)

    batch_size = Nx.axis_size(z_a, 0)

    # Cross-correlation matrix: [dim, dim]
    # C = (1/N) * z_a^T @ z_b
    c = Nx.dot(Nx.transpose(z_a_norm), z_b_norm) / batch_size

    dim = Nx.axis_size(c, 0)

    # Invariance term: sum_i (1 - C_ii)^2
    diag = Nx.take_diagonal(c)
    invariance = Nx.sum(Nx.pow(1.0 - diag, 2))

    # Redundancy reduction: sum_{i!=j} C_ij^2
    identity = Nx.eye(dim)
    off_diag_mask = 1.0 - identity
    redundancy = Nx.sum(Nx.pow(c * off_diag_mask, 2))

    invariance + lambda * redundancy
  end

  defnp batch_normalize(z) do
    mean = Nx.mean(z, axes: [0], keep_axes: true)
    std = Nx.sqrt(Nx.variance(z, axes: [0], keep_axes: true) + 1.0e-5)
    (z - mean) / std
  end

  # ============================================================================
  # Utilities
  # ============================================================================

  @doc """
  Get the output size of the Barlow Twins model.
  """
  @spec output_size(keyword()) :: non_neg_integer()
  def output_size(opts \\ []) do
    Keyword.get(opts, :projection_dim, default_projection_dim())
  end
end
