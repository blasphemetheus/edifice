# Edifice â€” Architecture TODO

## v0.2.0 (done)

- [x] Transformer (decoder-only) â€” GPT-style with GQA+RoPE+SwiGLU+RMSNorm
- [x] Mixture of Depths â€” Dynamic per-token compute allocation
- [x] RLHF/DPO Head â€” Reward model and preference heads
- [x] KAT â€” KAN + attention hybrid
- [x] mLSTM â€” Registry alias for xLSTM variant
- [x] RoPE option â€” Added to MultiHead and GQA
- [x] TTT variants â€” :linear and :mlp inner models
- [x] Based â€” Linear attention with Taylor expansion kernels
- [x] BitNet â€” Binary/ternary weight quantization
- [x] StripedHyena â€” Gated conv + Hyena hybrid
- [x] Mega â€” EMA + single-head gated attention
- [x] Conformer â€” Conv + Transformer for audio
- [x] FocalNet â€” Focal modulation for vision
- [x] PoolFormer â€” Pooling-based MetaFormer
- [x] NeRF â€” Positional encoding + MLP for radiance fields
- [x] GINv2 â€” GIN with edge features
- [x] Mixture of Agents â€” Multi-proposer + aggregator routing
- [x] RingAttention â€” Chunked attention with ring pattern
- [x] InfiniAttention â€” Compressive memory + local attention
- [x] CausalMask block â€” Unified mask creation
- [x] DepthwiseConv block â€” 1D depthwise separable convolution
- [x] TransformerBlock :custom_ffn â€” Callback for non-standard FFN

- [x] Mamba-3 â€” Complex states, trapezoidal discretization, MIMO rank-r
- [x] MLA â€” Multi-Head Latent Attention (DeepSeek-style KV compression)
- [x] JEPA â€” Joint Embedding Predictive Architecture (self-supervised)
- [x] Differential Transformer â€” Dual softmax attention with noise cancellation

## v0.3.0 (done)
- [x] **Hymba** â€” Hybrid Mamba+attention with learnable meta tokens
- [x] **sLSTM** â€” Scalar LSTM with exponential gating (xLSTM component)
- [x] **GSS** â€” Gated State Space (simplified S4 with multiplicative gating)
- [x] **Hawk/RecurrentGemma** â€” Google's RG-LRU recurrent model
- [x] **DiT v2** â€” Updated diffusion transformer with improved adaptive norm conditioning
- [x] **Mixture of Experts v2** â€” Expert choice routing, shared expert slots
- [x] **State Space Duality (SSD)** â€” Improved Mamba-2 structured masking
- [x] **xLSTM v2** â€” Updated mLSTM with matrix memory improvements
- [x] **Hyena v2** â€” Improved implicit long convolution filters
- [x] **RetNet v2** â€” Retention with improved chunkwise formulation
- [x] **MEGALODON** â€” Mega-scale sequence model (Meta)
- [x] **KV Cache support** â€” Inference-time KV caching for autoregressive models
- [ ] **Flash Attention** â€” IO-aware exact attention (requires EXLA backend work)
- [x] **Quantization toolkit** â€” GPTQ, AWQ, SqueezeLLM weight quantization
- [x] **LoRA+ / DoRA** â€” Improved low-rank adaptation variants

## 2026 Wave 1 (done)
- [x] **Gated DeltaNet** â€” Linear attention with data-dependent gating (Qwen3-Next, Kimi Linear)
- [x] **RWKV-7** â€” Generalized delta rule, "Goose" architecture
- [x] **TTT-E2E** â€” End-to-end test-time training
- [x] **MMDiT** â€” Multimodal Diffusion Transformer (FLUX.1, SD3)
- [x] **SoFlow** â€” Flow matching + consistency loss
- [x] **KDA** â€” Kimi Delta Attention (channel-wise decay)
- [x] **MambaVision** â€” 4-stage hierarchical CNN+Mamba+Attention
- [x] **Multimodal MLP Fusion** â€” MLP projection, cross-attention, Perceiver resampler
- [x] **RL Integration** â€” PPOTrainer, GAE, CartPole, GridWorld environments
- [x] **iRoPE** â€” Interleaved RoPE in decoder_only (Llama 4 pattern)
- [x] **Aux-loss-free MoE** â€” Bias-based load balancing in MoE v2

## 2026 Wave 2 (done)
- [x] **Gated Attention** â€” Sigmoid post-attention gate (NeurIPS 2025 best paper)
- [x] **NSA** â€” Native Sparse Attention (DeepSeek three-path)
- [x] **Scalable-Softmax** â€” Drop-in softmax replacement
- [x] **Softpick** â€” Non-saturating sparse attention function
- [x] **VAR** â€” Visual Autoregressive (next-scale prediction, NeurIPS 2024 best paper)
- [x] **Transfusion** â€” Unified AR text + diffusion images
- [x] **Linear DiT (SANA)** â€” Linear attention for diffusion
- [x] **SiT** â€” Scalable Interpolant Transformer
- [x] **MAR** â€” Masked Autoregressive generation
- [x] **DINOv2** â€” Self-distillation vision backbone
- [x] **MetaFormer / CAFormer** â€” Architecture-first framework
- [x] **EfficientViT** â€” Linear attention ViT
- [x] **SigLIP** â€” Sigmoid contrastive learning
- [x] **FNO** â€” Fourier Neural Operator (scientific ML)
- [x] **EGNN** â€” E(n)-Equivariant GNN
- [x] **DPO** â€” Direct Preference Optimization
- [x] **GRPO** â€” Group Relative Policy Optimization
- [x] **KTO** â€” Kahneman-Tversky Optimization
- [x] **Engram** â€” O(1) hash-based associative memory
- [x] **RNoPE-SWA** â€” No positional encoding + sliding window
- [x] **YaRN** â€” RoPE context extension
- [x] **Dual Chunk Attention** â€” Long-context chunked attention
- [x] **TMRoPE** â€” Time-aligned Multimodal RoPE
- [x] **Medusa** â€” Multi-head speculative decoding
- [x] **Gaussian Splatting** â€” 3D Gaussian Splatting (NeRF successor)
- [x] **TRELLIS** â€” Sparse 3D lattice generation
- [x] **CogVideoX** â€” 3D causal video generation
- [x] **ACT** â€” Action Chunking Transformer (robotics)
- [x] **OpenVLA** â€” Vision-Language-Action model
- [x] **EnCodec** â€” Neural audio codec
- [x] **VALL-E** â€” Codec language model for TTS
- [x] **SoundStorm** â€” Parallel audio token generation
- [x] **GGUF Export** â€” Model export to GGUF format

## 2026 Wave 3 â€” New Families & Gap Fills (done)

### Detection / Segmentation (new family)
- [x] **DETR** â€” DEtection TRansformer (set-based object detection with bipartite matching). Encoder-decoder transformer + learned object queries + Hungarian loss. Family: `detection`.
- [x] **RT-DETR** â€” Real-Time DETR (Baidu). Hybrid CNN+transformer encoder, anchor-free, NMS-free. 53-55% AP at 108 FPS. Practical real-time detection baseline.
- [x] **SAM 2** â€” Segment Anything Model 2 (Meta). Promptable segmentation for images + video. Image encoder + prompt encoder + mask decoder + memory attention for video. Major 2024/2025 release.

### Attention
- [x] **Sigmoid Self-Attention** â€” Drop-in softmax replacement using properly normalized sigmoid (ICLR 2025). FlashSigmoid yields 17% kernel speedup over FlashAttention2 on H100. Eliminates token competition. Standalone mechanism, distinct from Gated Attention's post-SDPA sigmoid gate.

### RL
- [x] **Decision Transformer** â€” Offline RL as conditional sequence generation (Chen et al. 2021). Frames RL as sequence modeling: conditions on desired return, state, action triples. Causal transformer predicts next action given (R, s, a) history. Directly relevant to ExPhil imitation learning pipeline.

### Audio
- [x] **Whisper** â€” Encoder-decoder ASR (OpenAI). Log-mel spectrogram frontend + transformer encoder-decoder with multitask training (transcription, translation, timestamps, language ID). Fills the ASR gap â€” audio family has TTS but no recognition.

### Generative
- [x] **Mercury/MDLM** â€” Discrete diffusion LM (Inception Labs, arXiv:2506.17298). Parallel token denoising instead of autoregressive generation. Transformer backbone + discrete noise process + iterative refinement. 10x decoding speedup. Related work: MDLM, SEDD, Plaid. New family: `diffusion_lm` or under `generative`.
- [ ] **Rectified Flow** â€” Straight-trajectory flow matching variant. ODE paths trained to be straight lines, enabling 10-100x fewer inference steps than vanilla diffusion. Can be a variant/option on existing FlowMatching or standalone module.

### Vision
- [ ] **DINOv3** â€” Self-supervised vision backbone (Meta AI, Aug 2025). CLIP-like image-text alignment + axial RoPE + Gram anchoring, scaled to 7B params. Major upgrade over DINOv2.

### Meta / Efficiency
- [ ] **EAGLE-3** â€” Multi-level speculative draft head. Extracts low/mid/high features from target model for multi-step draft prediction. 4-6x decoding speedup. Scaling law for speculative decoding.
- [ ] **ReMoE** â€” Fully differentiable MoE routing (ICLR 2025). Replaces discrete top-k with continuous relaxation via Gumbel-Softmax. Better gradient flow through routing.
- [ ] **mHC** â€” Manifold Hyper-Connections (DeepSeek-V4). Multi-rate residual streams.

### Graph
- [ ] **DimeNet** â€” Directional message passing with angle information between atoms. Important for molecular property prediction.
- [ ] **SE(3)-Transformer** â€” Equivariant transformer for structural biology.

### Remaining Candidates
- [ ] **Flash Attention** â€” IO-aware exact attention (requires EXLA backend work)
- [ ] **SPLA** â€” Sparse + Linear Attention hybrid
- [ ] **InfLLM-V2** â€” Block-partitioned KV cache selection
- [ ] **F5-TTS** â€” Non-autoregressive flow-matching TTS
- [ ] **JanusFlow** â€” AR text + rectified flow images
- [ ] **Show-o** â€” AR + discrete diffusion
- [ ] **Diffusion Policy** â€” Diffusion for robot action generation
- [ ] **CausVid** â€” Causal video DiT distillation
- [ ] **DeepONet** â€” Branch-trunk operator learning
- [ ] **MAGVIT-v2** â€” Lookup-free quantization for image/video tokens
- [ ] **MIRAS** â€” Google's Titans extension framework
- [ ] **MoR** â€” Mixture of Recursions
- [ ] **MoED** â€” Mixture of Expert Depths
- [ ] **Agent swarm patterns** â€” Multi-agent coordination framework
- [ ] **PointNet++** â€” Hierarchical point cloud processing
- [ ] **Wav2Vec 2.0** â€” Self-supervised speech backbone
- [ ] **Janus Multimodal** â€” Decoupled visual encoding for understanding + generation (CVPR 2025)
- [ ] **GPS** â€” General Powerful Scalable graph transformer

## ðŸ” Opus Review Pass â€” AI-Generated Architecture Implementations (2026-02-26)

All architectures added since Tier 1 (2026-02) were implemented by Claude Code (sonnet).
Reviewed by Opus for correctness, math accuracy, and idiomatic Elixir.

### Clean â€” no code changes needed (6/8)
- `lib/edifice/attention/nsa.ex` â€” 3-path sparse attention correct, proper 6-arg Nx.dot batching
- `lib/edifice/generative/transfusion.ex` â€” mixed AR+diffusion masking correct, dual heads + dual loss
- `lib/edifice/graph/egnn.ex` â€” equivariant coord update equations correct, proper Nx.dot batching
- `lib/edifice/memory/engram.ex` â€” LSH hashing via sign-based binary projection correct, EMA sound
- `lib/edifice/attention/yarn.ex` â€” wavelength-based frequency scaling correct, norm-preserving RoPE
- `lib/edifice/scientific/fno.ex` â€” spectral convolution correct; O(n^2) DFT matrix (Nx lacks FFT) known limitation

### Fixed (2/8)
- `lib/edifice/meta/moe_v2.ex` â€” stack_fn fallback was broken for non-standard expert counts (3,5,6,7). Arity-1 generic closure incompatible with Axon.layer positional arg unpacking. Replaced with explicit cases for 2-8 experts.
- `lib/edifice/generative/var.ex` â€” token embedding used deterministic Nx.iota projection instead of learnable weights. Replaced with Axon.nx (one_hot) + Axon.dense (no bias) for proper learnable embedding table. Note: decoder reshape has a separate pre-existing bug (not addressed here).

---

## TransformerBlock Composability â€” Encoder-Decoder Support

**Problem:** `TransformerBlock.layer/2` only supports 2 sublayers (attention + FFN). Every encoder-decoder model in the codebase reimplements its own 3-sublayer decoder block (self-attn + cross-attn + FFN) with duplicated residual/norm/dropout wiring. This is the single largest composability gap in Edifice.

**Affected modules (6 modules, ~300 lines of duplicated block structure):**
- `audio/whisper.ex` â€” `decoder_block/7` (self-attn + cross-attn + FFN)
- `robotics/act.ex` â€” `decoder_layer/6` (self-attn + cross-attn + FFN)
- `detection/detr.ex` â€” `decoder_layer/9` (self-attn + cross-attn + FFN, with per-layer PE)
- `detection/rt_detr.ex` â€” decoder with iterative bbox refinement
- `audio/valle.ex` â€” `decoder_block/6` (AR causal / NAR bidirectional modes)
- `detection/sam2.ex` â€” `two_way_block/8` (4 sublayers: self-attn + cross-attn + MLP + reverse cross-attn)

**Secondary issue â€” CrossAttention.layer adoption:** Only Whisper uses `CrossAttention.layer/3`. DETR, ACT, SAM2, Perceiver, and Fusion all implement custom inline cross-attention. These custom versions add module-specific features (per-layer PE, bidirectional conditioning, gated cross-attention) that `CrossAttention.layer/3` doesn't support.

**Proposed solution â€” extend TransformerBlock with optional cross-attention sublayer:**

```
TransformerBlock.layer(input, opts)                    # 2-sublayer (encoder, existing)
TransformerBlock.layer(input, memory, opts)             # 3-sublayer (decoder, new)
```

New `:cross_attention_fn` callback option (same pattern as existing `:attention_fn`):
```elixir
TransformerBlock.layer(x, memory,
  attention_fn: fn x, name -> causal_self_attn(x, name) end,
  cross_attention_fn: fn x, mem, name -> cross_attn(x, mem, name) end,
  hidden_size: 512,
  name: "dec_block_1"
)
```

When `cross_attention_fn` is provided, the block becomes:
```
norm â†’ attention_fn â†’ residual â†’ norm â†’ cross_attention_fn(x, memory) â†’ residual â†’ norm â†’ FFN â†’ residual
```

**Design considerations:**
- Backward compatible: 2-arg `layer/2` unchanged, 3-arg `layer/3` adds memory input
- `stack/3` gets a `stack/4` sibling for decoder stacking with shared memory
- Callback approach handles DETR's per-layer PE, SAM2's bidirectional, etc. â€” each module supplies its own cross-attention function
- `CrossAttention.layer/3` could gain `:num_heads` multi-head support (currently single-head despite taking `:num_heads`) to make the default callback more useful
- SAM2's 4-sublayer block (reverse cross-attn) stays custom â€” don't over-generalize

**Files to modify:**
- `lib/edifice/blocks/transformer_block.ex` â€” add `layer/3`, `stack/4`, cross-attention sublayer
- `lib/edifice/blocks/cross_attention.ex` â€” audit multi-head support, add callback-friendly API
- `lib/edifice/audio/whisper.ex` â€” refactor to use new `layer/3`
- `lib/edifice/robotics/act.ex` â€” refactor to use new `layer/3`
- `lib/edifice/detection/detr.ex` â€” refactor (PE callback captures positional encoding)
- `lib/edifice/audio/valle.ex` â€” evaluate refactor feasibility (AR/NAR split may need to stay custom)
- `test/edifice/blocks/transformer_block_test.exs` â€” add 3-sublayer tests

---

## CUDA Kernel Fusion for Recurrent Architectures - 2026-02-18 22:45

- **Explore fused RNN kernels for LSTM/GRU/minGRU/minLSTM** - Plan what's needed to make recurrent architectures competitive on GPU inference latency. **Problem:** Axon unrolls each recurrence timestep as separate EXLA kernel launches, causing 70-600ms latency for seq_len=32 vs 14ms for gated_ssm. TensorFlow/PyTorch use cuDNN's fused `cudnnRNNForward` kernel which handles all timesteps in one GPU call. **Files:** `bench/inference_latency.exs`, `lib/edifice/recurrent/lstm.ex`, `lib/edifice/recurrent/gru.ex`, `lib/edifice/recurrent/min_gru.ex`, `lib/edifice/recurrent/min_lstm.ex`. **Solution:** Investigate four approaches: (1) cuDNN fused RNN integration via EXLA/XLA custom calls, (2) custom CUDA kernels for fused LSTM cells callable from Nx, (3) XLA's built-in RNN fusion passes and whether EXLA exposes them, (4) step-by-step inference with explicit state passing (seq_len=1 per frame) which sidesteps unrolling entirely. Reference: slippi-ai achieves real-time LSTM inference via TensorFlow's cuDNN integration. Benchmark data in `tmp/bench_results/`.
