# Edifice ‚Äî Architecture TODO

## v0.2.0 (done)

- [x] Transformer (decoder-only) ‚Äî GPT-style with GQA+RoPE+SwiGLU+RMSNorm
- [x] Mixture of Depths ‚Äî Dynamic per-token compute allocation
- [x] RLHF/DPO Head ‚Äî Reward model and preference heads
- [x] KAT ‚Äî KAN + attention hybrid
- [x] mLSTM ‚Äî Registry alias for xLSTM variant
- [x] RoPE option ‚Äî Added to MultiHead and GQA
- [x] TTT variants ‚Äî :linear and :mlp inner models
- [x] Based ‚Äî Linear attention with Taylor expansion kernels
- [x] BitNet ‚Äî Binary/ternary weight quantization
- [x] StripedHyena ‚Äî Gated conv + Hyena hybrid
- [x] Mega ‚Äî EMA + single-head gated attention
- [x] Conformer ‚Äî Conv + Transformer for audio
- [x] FocalNet ‚Äî Focal modulation for vision
- [x] PoolFormer ‚Äî Pooling-based MetaFormer
- [x] NeRF ‚Äî Positional encoding + MLP for radiance fields
- [x] GINv2 ‚Äî GIN with edge features
- [x] Mixture of Agents ‚Äî Multi-proposer + aggregator routing
- [x] RingAttention ‚Äî Chunked attention with ring pattern
- [x] InfiniAttention ‚Äî Compressive memory + local attention
- [x] CausalMask block ‚Äî Unified mask creation
- [x] DepthwiseConv block ‚Äî 1D depthwise separable convolution
- [x] TransformerBlock :custom_ffn ‚Äî Callback for non-standard FFN

- [x] Mamba-3 ‚Äî Complex states, trapezoidal discretization, MIMO rank-r
- [x] MLA ‚Äî Multi-Head Latent Attention (DeepSeek-style KV compression)
- [x] JEPA ‚Äî Joint Embedding Predictive Architecture (self-supervised)
- [x] Differential Transformer ‚Äî Dual softmax attention with noise cancellation

## v0.3.0 Candidates
- [x] **Hymba** ‚Äî Hybrid Mamba+attention with learnable meta tokens
- [x] **sLSTM** ‚Äî Scalar LSTM with exponential gating (xLSTM component)
- [x] **GSS** ‚Äî Gated State Space (simplified S4 with multiplicative gating)
- [x] **Hawk/RecurrentGemma** ‚Äî Google's RG-LRU recurrent model
- [x] **DiT v2** ‚Äî Updated diffusion transformer with improved adaptive norm conditioning
- [x] **Mixture of Experts v2** ‚Äî Expert choice routing, shared expert slots
- [x] **State Space Duality (SSD)** ‚Äî Improved Mamba-2 structured masking
- [x] **xLSTM v2** ‚Äî Updated mLSTM with matrix memory improvements
- [x] **Hyena v2** ‚Äî Improved implicit long convolution filters
- [x] **RetNet v2** ‚Äî Retention with improved chunkwise formulation
- [x] **MEGALODON** ‚Äî Mega-scale sequence model (Meta)
- [x] **KV Cache support** ‚Äî Inference-time KV caching for autoregressive models
- [ ] **Flash Attention** ‚Äî IO-aware exact attention (requires EXLA backend work)
- [x] **Quantization toolkit** ‚Äî GPTQ, AWQ, SqueezeLLM weight quantization
- [x] **LoRA+ / DoRA** ‚Äî Improved low-rank adaptation variants

## üîç Opus Review Pass ‚Äî AI-Generated Architecture Implementations

All architectures added since Tier 1 (2026-02) were implemented by Claude Code (sonnet).
**Bradley to review with Opus for correctness, math accuracy, and idiomatic Elixir.**

Priority review targets (most complex / most novel):
- `lib/edifice/attention/nsa.ex` ‚Äî NSA three-path sparse attention (complex)
- `lib/edifice/generative/transfusion.ex` ‚Äî mixed AR+diffusion mask logic
- `lib/edifice/generative/var.ex` ‚Äî next-scale VQ tokenizer
- `lib/edifice/scientific/fno.ex` ‚Äî spectral convolution via FFT
- `lib/edifice/graph/egnn.ex` ‚Äî equivariant coord update equations
- `lib/edifice/memory/engram.ex` ‚Äî LSH hash routing
- `lib/edifice/attention/yarn.ex` ‚Äî RoPE frequency band interpolation
- `lib/edifice/meta/moe_v2.ex` ‚Äî bias-based load balancing

Suggested process:
1. Open each file + the reference paper
2. Ask Opus: "Verify this Elixir implementation matches the paper's equations. Flag any math errors, missing features, or non-idiomatic patterns."
3. Commit verified files with `verified: true` in @moduledoc metadata

---

## CUDA Kernel Fusion for Recurrent Architectures - 2026-02-18 22:45

- **Explore fused RNN kernels for LSTM/GRU/minGRU/minLSTM** - Plan what's needed to make recurrent architectures competitive on GPU inference latency. **Problem:** Axon unrolls each recurrence timestep as separate EXLA kernel launches, causing 70-600ms latency for seq_len=32 vs 14ms for gated_ssm. TensorFlow/PyTorch use cuDNN's fused `cudnnRNNForward` kernel which handles all timesteps in one GPU call. **Files:** `bench/inference_latency.exs`, `lib/edifice/recurrent/lstm.ex`, `lib/edifice/recurrent/gru.ex`, `lib/edifice/recurrent/min_gru.ex`, `lib/edifice/recurrent/min_lstm.ex`. **Solution:** Investigate four approaches: (1) cuDNN fused RNN integration via EXLA/XLA custom calls, (2) custom CUDA kernels for fused LSTM cells callable from Nx, (3) XLA's built-in RNN fusion passes and whether EXLA exposes them, (4) step-by-step inference with explicit state passing (seq_len=1 per frame) which sidesteps unrolling entirely. Reference: slippi-ai achieves real-time LSTM inference via TensorFlow's cuDNN integration. Benchmark data in `tmp/bench_results/`.
