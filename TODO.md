# Edifice ‚Äî Architecture TODO

## v0.2.0 (done)

- [x] Transformer (decoder-only) ‚Äî GPT-style with GQA+RoPE+SwiGLU+RMSNorm
- [x] Mixture of Depths ‚Äî Dynamic per-token compute allocation
- [x] RLHF/DPO Head ‚Äî Reward model and preference heads
- [x] KAT ‚Äî KAN + attention hybrid
- [x] mLSTM ‚Äî Registry alias for xLSTM variant
- [x] RoPE option ‚Äî Added to MultiHead and GQA
- [x] TTT variants ‚Äî :linear and :mlp inner models
- [x] Based ‚Äî Linear attention with Taylor expansion kernels
- [x] BitNet ‚Äî Binary/ternary weight quantization
- [x] StripedHyena ‚Äî Gated conv + Hyena hybrid
- [x] Mega ‚Äî EMA + single-head gated attention
- [x] Conformer ‚Äî Conv + Transformer for audio
- [x] FocalNet ‚Äî Focal modulation for vision
- [x] PoolFormer ‚Äî Pooling-based MetaFormer
- [x] NeRF ‚Äî Positional encoding + MLP for radiance fields
- [x] GINv2 ‚Äî GIN with edge features
- [x] Mixture of Agents ‚Äî Multi-proposer + aggregator routing
- [x] RingAttention ‚Äî Chunked attention with ring pattern
- [x] InfiniAttention ‚Äî Compressive memory + local attention
- [x] CausalMask block ‚Äî Unified mask creation
- [x] DepthwiseConv block ‚Äî 1D depthwise separable convolution
- [x] TransformerBlock :custom_ffn ‚Äî Callback for non-standard FFN

- [x] Mamba-3 ‚Äî Complex states, trapezoidal discretization, MIMO rank-r
- [x] MLA ‚Äî Multi-Head Latent Attention (DeepSeek-style KV compression)
- [x] JEPA ‚Äî Joint Embedding Predictive Architecture (self-supervised)
- [x] Differential Transformer ‚Äî Dual softmax attention with noise cancellation

## v0.3.0 (done)
- [x] **Hymba** ‚Äî Hybrid Mamba+attention with learnable meta tokens
- [x] **sLSTM** ‚Äî Scalar LSTM with exponential gating (xLSTM component)
- [x] **GSS** ‚Äî Gated State Space (simplified S4 with multiplicative gating)
- [x] **Hawk/RecurrentGemma** ‚Äî Google's RG-LRU recurrent model
- [x] **DiT v2** ‚Äî Updated diffusion transformer with improved adaptive norm conditioning
- [x] **Mixture of Experts v2** ‚Äî Expert choice routing, shared expert slots
- [x] **State Space Duality (SSD)** ‚Äî Improved Mamba-2 structured masking
- [x] **xLSTM v2** ‚Äî Updated mLSTM with matrix memory improvements
- [x] **Hyena v2** ‚Äî Improved implicit long convolution filters
- [x] **RetNet v2** ‚Äî Retention with improved chunkwise formulation
- [x] **MEGALODON** ‚Äî Mega-scale sequence model (Meta)
- [x] **KV Cache support** ‚Äî Inference-time KV caching for autoregressive models
- [ ] **Flash Attention** ‚Äî IO-aware exact attention (requires EXLA backend work)
- [x] **Quantization toolkit** ‚Äî GPTQ, AWQ, SqueezeLLM weight quantization
- [x] **LoRA+ / DoRA** ‚Äî Improved low-rank adaptation variants

## 2026 Wave 1 (done)
- [x] **Gated DeltaNet** ‚Äî Linear attention with data-dependent gating (Qwen3-Next, Kimi Linear)
- [x] **RWKV-7** ‚Äî Generalized delta rule, "Goose" architecture
- [x] **TTT-E2E** ‚Äî End-to-end test-time training
- [x] **MMDiT** ‚Äî Multimodal Diffusion Transformer (FLUX.1, SD3)
- [x] **SoFlow** ‚Äî Flow matching + consistency loss
- [x] **KDA** ‚Äî Kimi Delta Attention (channel-wise decay)
- [x] **MambaVision** ‚Äî 4-stage hierarchical CNN+Mamba+Attention
- [x] **Multimodal MLP Fusion** ‚Äî MLP projection, cross-attention, Perceiver resampler
- [x] **RL Integration** ‚Äî PPOTrainer, GAE, CartPole, GridWorld environments
- [x] **iRoPE** ‚Äî Interleaved RoPE in decoder_only (Llama 4 pattern)
- [x] **Aux-loss-free MoE** ‚Äî Bias-based load balancing in MoE v2

## 2026 Wave 2 (done)
- [x] **Gated Attention** ‚Äî Sigmoid post-attention gate (NeurIPS 2025 best paper)
- [x] **NSA** ‚Äî Native Sparse Attention (DeepSeek three-path)
- [x] **Scalable-Softmax** ‚Äî Drop-in softmax replacement
- [x] **Softpick** ‚Äî Non-saturating sparse attention function
- [x] **VAR** ‚Äî Visual Autoregressive (next-scale prediction, NeurIPS 2024 best paper)
- [x] **Transfusion** ‚Äî Unified AR text + diffusion images
- [x] **Linear DiT (SANA)** ‚Äî Linear attention for diffusion
- [x] **SiT** ‚Äî Scalable Interpolant Transformer
- [x] **MAR** ‚Äî Masked Autoregressive generation
- [x] **DINOv2** ‚Äî Self-distillation vision backbone
- [x] **MetaFormer / CAFormer** ‚Äî Architecture-first framework
- [x] **EfficientViT** ‚Äî Linear attention ViT
- [x] **SigLIP** ‚Äî Sigmoid contrastive learning
- [x] **FNO** ‚Äî Fourier Neural Operator (scientific ML)
- [x] **EGNN** ‚Äî E(n)-Equivariant GNN
- [x] **DPO** ‚Äî Direct Preference Optimization
- [x] **GRPO** ‚Äî Group Relative Policy Optimization
- [x] **KTO** ‚Äî Kahneman-Tversky Optimization
- [x] **Engram** ‚Äî O(1) hash-based associative memory
- [x] **RNoPE-SWA** ‚Äî No positional encoding + sliding window
- [x] **YaRN** ‚Äî RoPE context extension
- [x] **Dual Chunk Attention** ‚Äî Long-context chunked attention
- [x] **TMRoPE** ‚Äî Time-aligned Multimodal RoPE
- [x] **Medusa** ‚Äî Multi-head speculative decoding
- [x] **Gaussian Splatting** ‚Äî 3D Gaussian Splatting (NeRF successor)
- [x] **TRELLIS** ‚Äî Sparse 3D lattice generation
- [x] **CogVideoX** ‚Äî 3D causal video generation
- [x] **ACT** ‚Äî Action Chunking Transformer (robotics)
- [x] **OpenVLA** ‚Äî Vision-Language-Action model
- [x] **EnCodec** ‚Äî Neural audio codec
- [x] **VALL-E** ‚Äî Codec language model for TTS
- [x] **SoundStorm** ‚Äî Parallel audio token generation
- [x] **GGUF Export** ‚Äî Model export to GGUF format

## Remaining Candidates
- [ ] **Flash Attention** ‚Äî IO-aware exact attention (requires EXLA backend work)
- [ ] **SPLA** ‚Äî Sparse + Linear Attention hybrid
- [ ] **InfLLM-V2** ‚Äî Block-partitioned KV cache selection
- [ ] **F5-TTS** ‚Äî Non-autoregressive flow-matching TTS
- [ ] **JanusFlow** ‚Äî AR text + rectified flow images
- [ ] **Show-o** ‚Äî AR + discrete diffusion
- [ ] **Diffusion Policy** ‚Äî Diffusion for robot action generation
- [ ] **CausVid** ‚Äî Causal video DiT distillation
- [ ] **DeepONet** ‚Äî Branch-trunk operator learning
- [ ] **SE(3)-Transformer** ‚Äî Equivariant transformer for structural biology
- [ ] **MAGVIT-v2** ‚Äî Lookup-free quantization for image/video tokens
- [ ] **mHC** ‚Äî Manifold Hyper-Connections (DeepSeek-V4)
- [ ] **MIRAS** ‚Äî Google's Titans extension framework
- [ ] **MoR** ‚Äî Mixture of Recursions
- [ ] **MoED** ‚Äî Mixture of Expert Depths
- [ ] **Agent swarm patterns** ‚Äî Multi-agent coordination framework

## üîç Opus Review Pass ‚Äî AI-Generated Architecture Implementations

All architectures added since Tier 1 (2026-02) were implemented by Claude Code (sonnet).
**Bradley to review with Opus for correctness, math accuracy, and idiomatic Elixir.**

Priority review targets (most complex / most novel):
- `lib/edifice/attention/nsa.ex` ‚Äî NSA three-path sparse attention (complex)
- `lib/edifice/generative/transfusion.ex` ‚Äî mixed AR+diffusion mask logic
- `lib/edifice/generative/var.ex` ‚Äî next-scale VQ tokenizer
- `lib/edifice/scientific/fno.ex` ‚Äî spectral convolution via FFT
- `lib/edifice/graph/egnn.ex` ‚Äî equivariant coord update equations
- `lib/edifice/memory/engram.ex` ‚Äî LSH hash routing
- `lib/edifice/attention/yarn.ex` ‚Äî RoPE frequency band interpolation
- `lib/edifice/meta/moe_v2.ex` ‚Äî bias-based load balancing

Suggested process:
1. Open each file + the reference paper
2. Ask Opus: "Verify this Elixir implementation matches the paper's equations. Flag any math errors, missing features, or non-idiomatic patterns."
3. Commit verified files with `verified: true` in @moduledoc metadata

---

## CUDA Kernel Fusion for Recurrent Architectures - 2026-02-18 22:45

- **Explore fused RNN kernels for LSTM/GRU/minGRU/minLSTM** - Plan what's needed to make recurrent architectures competitive on GPU inference latency. **Problem:** Axon unrolls each recurrence timestep as separate EXLA kernel launches, causing 70-600ms latency for seq_len=32 vs 14ms for gated_ssm. TensorFlow/PyTorch use cuDNN's fused `cudnnRNNForward` kernel which handles all timesteps in one GPU call. **Files:** `bench/inference_latency.exs`, `lib/edifice/recurrent/lstm.ex`, `lib/edifice/recurrent/gru.ex`, `lib/edifice/recurrent/min_gru.ex`, `lib/edifice/recurrent/min_lstm.ex`. **Solution:** Investigate four approaches: (1) cuDNN fused RNN integration via EXLA/XLA custom calls, (2) custom CUDA kernels for fused LSTM cells callable from Nx, (3) XLA's built-in RNN fusion passes and whether EXLA exposes them, (4) step-by-step inference with explicit state passing (seq_len=1 per frame) which sidesteps unrolling entirely. Reference: slippi-ai achieves real-time LSTM inference via TensorFlow's cuDNN integration. Benchmark data in `tmp/bench_results/`.
