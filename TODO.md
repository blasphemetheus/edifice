# Edifice ‚Äî Architecture TODO

## v0.2.0 (done)

- [x] Transformer (decoder-only) ‚Äî GPT-style with GQA+RoPE+SwiGLU+RMSNorm
- [x] Mixture of Depths ‚Äî Dynamic per-token compute allocation
- [x] RLHF/DPO Head ‚Äî Reward model and preference heads
- [x] KAT ‚Äî KAN + attention hybrid
- [x] mLSTM ‚Äî Registry alias for xLSTM variant
- [x] RoPE option ‚Äî Added to MultiHead and GQA
- [x] TTT variants ‚Äî :linear and :mlp inner models
- [x] Based ‚Äî Linear attention with Taylor expansion kernels
- [x] BitNet ‚Äî Binary/ternary weight quantization
- [x] StripedHyena ‚Äî Gated conv + Hyena hybrid
- [x] Mega ‚Äî EMA + single-head gated attention
- [x] Conformer ‚Äî Conv + Transformer for audio
- [x] FocalNet ‚Äî Focal modulation for vision
- [x] PoolFormer ‚Äî Pooling-based MetaFormer
- [x] NeRF ‚Äî Positional encoding + MLP for radiance fields
- [x] GINv2 ‚Äî GIN with edge features
- [x] Mixture of Agents ‚Äî Multi-proposer + aggregator routing
- [x] RingAttention ‚Äî Chunked attention with ring pattern
- [x] InfiniAttention ‚Äî Compressive memory + local attention
- [x] CausalMask block ‚Äî Unified mask creation
- [x] DepthwiseConv block ‚Äî 1D depthwise separable convolution
- [x] TransformerBlock :custom_ffn ‚Äî Callback for non-standard FFN

- [x] Mamba-3 ‚Äî Complex states, trapezoidal discretization, MIMO rank-r
- [x] MLA ‚Äî Multi-Head Latent Attention (DeepSeek-style KV compression)
- [x] JEPA ‚Äî Joint Embedding Predictive Architecture (self-supervised)
- [x] Differential Transformer ‚Äî Dual softmax attention with noise cancellation

## v0.3.0 (done)
- [x] **Hymba** ‚Äî Hybrid Mamba+attention with learnable meta tokens
- [x] **sLSTM** ‚Äî Scalar LSTM with exponential gating (xLSTM component)
- [x] **GSS** ‚Äî Gated State Space (simplified S4 with multiplicative gating)
- [x] **Hawk/RecurrentGemma** ‚Äî Google's RG-LRU recurrent model
- [x] **DiT v2** ‚Äî Updated diffusion transformer with improved adaptive norm conditioning
- [x] **Mixture of Experts v2** ‚Äî Expert choice routing, shared expert slots
- [x] **State Space Duality (SSD)** ‚Äî Improved Mamba-2 structured masking
- [x] **xLSTM v2** ‚Äî Updated mLSTM with matrix memory improvements
- [x] **Hyena v2** ‚Äî Improved implicit long convolution filters
- [x] **RetNet v2** ‚Äî Retention with improved chunkwise formulation
- [x] **MEGALODON** ‚Äî Mega-scale sequence model (Meta)
- [x] **KV Cache support** ‚Äî Inference-time KV caching for autoregressive models
- [ ] **Flash Attention** ‚Äî IO-aware exact attention (requires EXLA backend work)
- [x] **Quantization toolkit** ‚Äî GPTQ, AWQ, SqueezeLLM weight quantization
- [x] **LoRA+ / DoRA** ‚Äî Improved low-rank adaptation variants

## 2026 Wave 1 (done)
- [x] **Gated DeltaNet** ‚Äî Linear attention with data-dependent gating (Qwen3-Next, Kimi Linear)
- [x] **RWKV-7** ‚Äî Generalized delta rule, "Goose" architecture
- [x] **TTT-E2E** ‚Äî End-to-end test-time training
- [x] **MMDiT** ‚Äî Multimodal Diffusion Transformer (FLUX.1, SD3)
- [x] **SoFlow** ‚Äî Flow matching + consistency loss
- [x] **KDA** ‚Äî Kimi Delta Attention (channel-wise decay)
- [x] **MambaVision** ‚Äî 4-stage hierarchical CNN+Mamba+Attention
- [x] **Multimodal MLP Fusion** ‚Äî MLP projection, cross-attention, Perceiver resampler
- [x] **RL Integration** ‚Äî PPOTrainer, GAE, CartPole, GridWorld environments
- [x] **iRoPE** ‚Äî Interleaved RoPE in decoder_only (Llama 4 pattern)
- [x] **Aux-loss-free MoE** ‚Äî Bias-based load balancing in MoE v2

## 2026 Wave 2 (done)
- [x] **Gated Attention** ‚Äî Sigmoid post-attention gate (NeurIPS 2025 best paper)
- [x] **NSA** ‚Äî Native Sparse Attention (DeepSeek three-path)
- [x] **Scalable-Softmax** ‚Äî Drop-in softmax replacement
- [x] **Softpick** ‚Äî Non-saturating sparse attention function
- [x] **VAR** ‚Äî Visual Autoregressive (next-scale prediction, NeurIPS 2024 best paper)
- [x] **Transfusion** ‚Äî Unified AR text + diffusion images
- [x] **Linear DiT (SANA)** ‚Äî Linear attention for diffusion
- [x] **SiT** ‚Äî Scalable Interpolant Transformer
- [x] **MAR** ‚Äî Masked Autoregressive generation
- [x] **DINOv2** ‚Äî Self-distillation vision backbone
- [x] **MetaFormer / CAFormer** ‚Äî Architecture-first framework
- [x] **EfficientViT** ‚Äî Linear attention ViT
- [x] **SigLIP** ‚Äî Sigmoid contrastive learning
- [x] **FNO** ‚Äî Fourier Neural Operator (scientific ML)
- [x] **EGNN** ‚Äî E(n)-Equivariant GNN
- [x] **DPO** ‚Äî Direct Preference Optimization
- [x] **GRPO** ‚Äî Group Relative Policy Optimization
- [x] **KTO** ‚Äî Kahneman-Tversky Optimization
- [x] **Engram** ‚Äî O(1) hash-based associative memory
- [x] **RNoPE-SWA** ‚Äî No positional encoding + sliding window
- [x] **YaRN** ‚Äî RoPE context extension
- [x] **Dual Chunk Attention** ‚Äî Long-context chunked attention
- [x] **TMRoPE** ‚Äî Time-aligned Multimodal RoPE
- [x] **Medusa** ‚Äî Multi-head speculative decoding
- [x] **Gaussian Splatting** ‚Äî 3D Gaussian Splatting (NeRF successor)
- [x] **TRELLIS** ‚Äî Sparse 3D lattice generation
- [x] **CogVideoX** ‚Äî 3D causal video generation
- [x] **ACT** ‚Äî Action Chunking Transformer (robotics)
- [x] **OpenVLA** ‚Äî Vision-Language-Action model
- [x] **EnCodec** ‚Äî Neural audio codec
- [x] **VALL-E** ‚Äî Codec language model for TTS
- [x] **SoundStorm** ‚Äî Parallel audio token generation
- [x] **GGUF Export** ‚Äî Model export to GGUF format

## 2026 Wave 3 ‚Äî New Families & Gap Fills

### Detection / Segmentation (new family)
- [ ] **DETR** ‚Äî DEtection TRansformer (set-based object detection with bipartite matching). Encoder-decoder transformer + learned object queries + Hungarian loss. Family: `detection`.
- [ ] **RT-DETR** ‚Äî Real-Time DETR (Baidu). Hybrid CNN+transformer encoder, anchor-free, NMS-free. 53-55% AP at 108 FPS. Practical real-time detection baseline.
- [ ] **SAM 2** ‚Äî Segment Anything Model 2 (Meta). Promptable segmentation for images + video. Image encoder + prompt encoder + mask decoder + memory attention for video. Major 2024/2025 release.

### Attention
- [ ] **Sigmoid Self-Attention** ‚Äî Drop-in softmax replacement using properly normalized sigmoid (ICLR 2025). FlashSigmoid yields 17% kernel speedup over FlashAttention2 on H100. Eliminates token competition. Standalone mechanism, distinct from Gated Attention's post-SDPA sigmoid gate.

### RL
- [ ] **Decision Transformer** ‚Äî Offline RL as conditional sequence generation (Chen et al. 2021). Frames RL as sequence modeling: conditions on desired return, state, action triples. Causal transformer predicts next action given (R, s, a) history. Directly relevant to ExPhil imitation learning pipeline.

### Audio
- [ ] **Whisper** ‚Äî Encoder-decoder ASR (OpenAI). Log-mel spectrogram frontend + transformer encoder-decoder with multitask training (transcription, translation, timestamps, language ID). Fills the ASR gap ‚Äî audio family has TTS but no recognition.

### Generative
- [ ] **Mercury/MDLM** ‚Äî Discrete diffusion LM (Inception Labs, arXiv:2506.17298). Parallel token denoising instead of autoregressive generation. Transformer backbone + discrete noise process + iterative refinement. 10x decoding speedup. Related work: MDLM, SEDD, Plaid. New family: `diffusion_lm` or under `generative`.
- [ ] **Rectified Flow** ‚Äî Straight-trajectory flow matching variant. ODE paths trained to be straight lines, enabling 10-100x fewer inference steps than vanilla diffusion. Can be a variant/option on existing FlowMatching or standalone module.

### Vision
- [ ] **DINOv3** ‚Äî Self-supervised vision backbone (Meta AI, Aug 2025). CLIP-like image-text alignment + axial RoPE + Gram anchoring, scaled to 7B params. Major upgrade over DINOv2.

### Meta / Efficiency
- [ ] **EAGLE-3** ‚Äî Multi-level speculative draft head. Extracts low/mid/high features from target model for multi-step draft prediction. 4-6x decoding speedup. Scaling law for speculative decoding.
- [ ] **ReMoE** ‚Äî Fully differentiable MoE routing (ICLR 2025). Replaces discrete top-k with continuous relaxation via Gumbel-Softmax. Better gradient flow through routing.
- [ ] **mHC** ‚Äî Manifold Hyper-Connections (DeepSeek-V4). Multi-rate residual streams.

### Graph
- [ ] **DimeNet** ‚Äî Directional message passing with angle information between atoms. Important for molecular property prediction.
- [ ] **SE(3)-Transformer** ‚Äî Equivariant transformer for structural biology.

### Remaining Candidates
- [ ] **Flash Attention** ‚Äî IO-aware exact attention (requires EXLA backend work)
- [ ] **SPLA** ‚Äî Sparse + Linear Attention hybrid
- [ ] **InfLLM-V2** ‚Äî Block-partitioned KV cache selection
- [ ] **F5-TTS** ‚Äî Non-autoregressive flow-matching TTS
- [ ] **JanusFlow** ‚Äî AR text + rectified flow images
- [ ] **Show-o** ‚Äî AR + discrete diffusion
- [ ] **Diffusion Policy** ‚Äî Diffusion for robot action generation
- [ ] **CausVid** ‚Äî Causal video DiT distillation
- [ ] **DeepONet** ‚Äî Branch-trunk operator learning
- [ ] **MAGVIT-v2** ‚Äî Lookup-free quantization for image/video tokens
- [ ] **MIRAS** ‚Äî Google's Titans extension framework
- [ ] **MoR** ‚Äî Mixture of Recursions
- [ ] **MoED** ‚Äî Mixture of Expert Depths
- [ ] **Agent swarm patterns** ‚Äî Multi-agent coordination framework
- [ ] **PointNet++** ‚Äî Hierarchical point cloud processing
- [ ] **Wav2Vec 2.0** ‚Äî Self-supervised speech backbone
- [ ] **Janus Multimodal** ‚Äî Decoupled visual encoding for understanding + generation (CVPR 2025)
- [ ] **GPS** ‚Äî General Powerful Scalable graph transformer

## üîç Opus Review Pass ‚Äî AI-Generated Architecture Implementations (2026-02-26)

All architectures added since Tier 1 (2026-02) were implemented by Claude Code (sonnet).
Reviewed by Opus for correctness, math accuracy, and idiomatic Elixir.

### Clean ‚Äî no code changes needed (6/8)
- `lib/edifice/attention/nsa.ex` ‚Äî 3-path sparse attention correct, proper 6-arg Nx.dot batching
- `lib/edifice/generative/transfusion.ex` ‚Äî mixed AR+diffusion masking correct, dual heads + dual loss
- `lib/edifice/graph/egnn.ex` ‚Äî equivariant coord update equations correct, proper Nx.dot batching
- `lib/edifice/memory/engram.ex` ‚Äî LSH hashing via sign-based binary projection correct, EMA sound
- `lib/edifice/attention/yarn.ex` ‚Äî wavelength-based frequency scaling correct, norm-preserving RoPE
- `lib/edifice/scientific/fno.ex` ‚Äî spectral convolution correct; O(n^2) DFT matrix (Nx lacks FFT) known limitation

### Fixed (2/8)
- `lib/edifice/meta/moe_v2.ex` ‚Äî stack_fn fallback was broken for non-standard expert counts (3,5,6,7). Arity-1 generic closure incompatible with Axon.layer positional arg unpacking. Replaced with explicit cases for 2-8 experts.
- `lib/edifice/generative/var.ex` ‚Äî token embedding used deterministic Nx.iota projection instead of learnable weights. Replaced with Axon.nx (one_hot) + Axon.dense (no bias) for proper learnable embedding table. Note: decoder reshape has a separate pre-existing bug (not addressed here).

---

## CUDA Kernel Fusion for Recurrent Architectures - 2026-02-18 22:45

- **Explore fused RNN kernels for LSTM/GRU/minGRU/minLSTM** - Plan what's needed to make recurrent architectures competitive on GPU inference latency. **Problem:** Axon unrolls each recurrence timestep as separate EXLA kernel launches, causing 70-600ms latency for seq_len=32 vs 14ms for gated_ssm. TensorFlow/PyTorch use cuDNN's fused `cudnnRNNForward` kernel which handles all timesteps in one GPU call. **Files:** `bench/inference_latency.exs`, `lib/edifice/recurrent/lstm.ex`, `lib/edifice/recurrent/gru.ex`, `lib/edifice/recurrent/min_gru.ex`, `lib/edifice/recurrent/min_lstm.ex`. **Solution:** Investigate four approaches: (1) cuDNN fused RNN integration via EXLA/XLA custom calls, (2) custom CUDA kernels for fused LSTM cells callable from Nx, (3) XLA's built-in RNN fusion passes and whether EXLA exposes them, (4) step-by-step inference with explicit state passing (seq_len=1 per frame) which sidesteps unrolling entirely. Reference: slippi-ai achieves real-time LSTM inference via TensorFlow's cuDNN integration. Benchmark data in `tmp/bench_results/`.
